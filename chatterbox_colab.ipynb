{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["# Chatterbox TTS - Colab Notebook\n",
    "This notebook demonstrates how to use Chatterbox, a state-of-the-art open-source TTS model with IndexTTS-inspired features. Follow the steps below to get started.\n",
    "\n",
    "## ðŸš¨ IMPORTANT: If you get 'generate_long_text' AttributeError\n",
    "\n",
    "If you see an error like `'ChatterboxTTS' object has no attribute 'generate_long_text'`, this is due to Python's import cache. **SOLUTION:**\n",
    "\n",
    "1. **RESTART YOUR KERNEL** (Runtime â†’ Restart Runtime in Colab)\n",
    "2. Run the \"Clear Import Cache\" cell below\n",
    "3. Re-run all cells in order\n",
    "\n",
    "The `generate_long_text` method exists and works - it's just a caching issue!"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 1. Install Dependencies\n",
    "First, let's install the required packages."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["!pip install chatterbox-tts gradio"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 2. Clear Import Cache (IMPORTANT!)\n",
    "If you're getting 'generate_long_text' attribute errors, run this cell first to clear Python's import cache."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Clear Python import cache to ensure we get the latest version\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# Remove cached chatterbox modules\n",
    "modules_to_remove = [key for key in sys.modules.keys() if key.startswith('chatterbox')]\n",
    "for module in modules_to_remove:\n",
    "    del sys.modules[module]\n",
    "    print(f\"Removed cached module: {module}\")\n",
    "\n",
    "print(\"âœ… Import cache cleared successfully!\")"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 3. Import Libraries and Setup"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["import sys\n",
    "import importlib\n",
    "import torch\n",
    "import torchaudio as ta\n",
    "\n",
    "# Comprehensive module cache clearing for Colab\n",
    "print(\"ðŸ”„ Clearing all chatterbox modules from cache...\")\n",
    "modules_to_remove = [key for key in sys.modules.keys() if 'chatterbox' in key.lower()]\n",
    "for module in modules_to_remove:\n",
    "    if module in sys.modules:\n",
    "        del sys.modules[module]\n",
    "        print(f\"   Removed: {module}\")\n",
    "\n",
    "# Force fresh import\n",
    "try:\n",
    "    import chatterbox\n",
    "    importlib.reload(chatterbox)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    import chatterbox.tts\n",
    "    importlib.reload(chatterbox.tts)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Final import\n",
    "from chatterbox.tts import ChatterboxTTS\n",
    "print(\"âœ… ChatterboxTTS imported successfully!\")\n",
    "\n",
    "# Automatically detect the best available device with detailed info\n",
     "if torch.cuda.is_available():\n",
     "    device = \"cuda\"\n",
     "    print(f\"ðŸš€ Using GPU: {torch.cuda.get_device_name(0)}\")\n",
     "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
     "    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
     "    print(f\"   PyTorch Version: {torch.__version__}\")\n",
     "elif torch.backends.mps.is_available():\n",
     "    device = \"mps\"\n",
     "    print(\"ðŸŽ Using Apple Metal Performance Shaders (MPS)\")\n",
     "else:\n",
     "    device = \"cpu\"\n",
     "    print(\"âš ï¸  Using CPU (No GPU/MPS available)\")\n",
     "    print(\"   For better performance, ensure CUDA (NVIDIA) or MPS (Apple) is available\")\n",
     "\n",
     "print(f\"\\nDevice: {device}\")\n",
     "\n",
     "# Comprehensive method verification\n",
     "print(\"\\nðŸ” Verifying ChatterboxTTS methods...\")\n",
     "methods_to_check = ['generate', 'generate_long_text', 'generate_streaming', 'estimate_memory_usage']\n",
     "for method_name in methods_to_check:\n",
     "    if hasattr(ChatterboxTTS, method_name):\n",
     "        print(f\"âœ… {method_name} method is available!\")\n",
     "    else:\n",
     "        print(f\"âŒ {method_name} method not found!\")\n",
     "\n",
     "# Additional verification for instance methods\n",
     "try:\n",
     "    import inspect\n",
     "    sig_long = inspect.signature(ChatterboxTTS.generate_long_text)\n",
     "    sig_estimate = inspect.signature(ChatterboxTTS.estimate_memory_usage)\n",
     "    print(f\"\\nðŸ“‹ Method signatures verified:\")\n",
     "    print(f\"   generate_long_text{sig_long}\")\n",
     "    print(f\"   estimate_memory_usage{sig_estimate}\")\n",
     "except Exception as e:\n",
     "    print(f\"âš ï¸  Could not verify method signatures: {e}\")\n",
     "\n",
     "print(\"\\nðŸŽ‰ Setup completed! Ready to load model.\")"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 4. Load the Model\n",
    "Now we'll load the pre-trained Chatterbox TTS model."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["model = ChatterboxTTS.from_pretrained(device=device)"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 5. Generate Speech from Text\n",
    "Let's generate some speech using the default voice."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["text = \"Hello! This is a test of the Chatterbox TTS system. It can generate natural-sounding speech from text.\"\n",
    "wav = model.generate(text)\n",
    "\n",
    "# Save the generated audio\n",
    "output_path = \"test_output.wav\"\n",
    "ta.save(output_path, wav, model.sr)\n",
    "\n",
    "# Display audio player in the notebook\n",
    "from IPython.display import Audio\n",
    "Audio(output_path)"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 6. Voice Cloning (Optional)\n",
    "You can also clone a voice by providing an audio prompt. Upload your audio file and specify its path below."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Upload your audio file and set its path here\n",
    "AUDIO_PROMPT_PATH = \"path_to_your_audio.wav\"  # Replace with your audio file path\n",
    "\n",
    "# Generate speech with the voice from the audio prompt\n",
    "text = \"This is the same text but spoken in a different voice.\"\n",
    "wav = model.generate(text, audio_prompt_path=AUDIO_PROMPT_PATH)\n",
    "\n",
    "# Save and play the generated audio\n",
    "output_path_cloned = \"test_output_cloned.wav\"\n",
    "ta.save(output_path_cloned, wav, model.sr)\n",
    "Audio(output_path_cloned)"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 7. Long Text Generation Example (IndexTTS-Inspired Features)\n",
    "This demonstrates the new generate_long_text method with advanced chunking and memory optimization."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Example of generating audio from long text with memory optimization\n", "long_text = \"\"\"\n", "This is a demonstration of the new long text generation capabilities. \n", "The system can now handle ultra-long texts by breaking them into smaller chunks. \n", "Each chunk is processed separately to avoid memory issues. \n", "The chunks are then combined to create seamless audio output. \n", "This approach allows for generating hours of audio content without running out of memory. \n", "The chunking can be done by sentences, clauses, or character count. \n", "Sentence-based chunking preserves natural speech patterns for the best quality. \n", "Memory optimization between chunks ensures efficient resource usage. \n", "You can also estimate memory usage before generation to plan accordingly.\n", "\"\"\"\n", "\n", "# Verify methods are available before using them\n", "print(\"ðŸ” Checking if model has required methods...\")\n", "\n", "if hasattr(model, 'estimate_memory_usage'):\n", "    print(\"âœ… estimate_memory_usage method found!\")\n", "    # Estimate memory usage\n", "    memory_info = model.estimate_memory_usage(long_text)\n", "    print(f\"Text length: {memory_info['text_length']} characters\")\n", "    print(f\"Estimated memory usage: {memory_info['total_estimated_mb']:.0f}MB\")\n", "    print(f\"Recommended chunk size: {memory_info['recommended_chunk_size']} characters\")\n", "else:\n", "    print(\"âŒ estimate_memory_usage method not found! Using fallback...\")\n", "    print(f\"Text length: {len(long_text)} characters\")\n", "    print(\"Estimated memory usage: ~50MB (fallback estimate)\")\n", "\n", "print(\"\\n\")\n", "\n", "if hasattr(model, 'generate_long_text'):\n", "    print(\"âœ… generate_long_text method found! Generating audio...\")\n", "    # Generate audio using long text method\n", "    wav_long = model.generate_long_text(\n", "        text=long_text,\n", "        chunk_method=\"sentences\",\n", "        max_chunk_size=200,\n", "        optimize_memory_between_chunks=True\n", "    )\n", "    \n", "    # Save and play the generated audio\n", "    output_path_long = \"test_output_long.wav\"\n", "    ta.save(output_path_long, wav_long, model.sr)\n", "    print(f\"âœ… Long audio saved to: {output_path_long}\")\n", "    Audio(output_path_long)\n", "else:\n", "    print(\"âŒ generate_long_text method not found! Using fallback generate method...\")\n", "    # Fallback to regular generate method\n", "    wav_long = model.generate(long_text)\n", "    \n", "    # Save and play the generated audio\n", "    output_path_long = \"test_output_long_fallback.wav\"\n", "    ta.save(output_path_long, wav_long, model.sr)\n", "    print(f\"âœ… Audio saved to: {output_path_long} (using fallback method)\")\n", "    Audio(output_path_long)"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 8. Gradio UI with Live Server\n",
    "Launch an interactive web interface with all the new features including long text generation."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["import gradio as gr\n", "import inspect\n", "\n", "def generate_with_gradio(text, audio_prompt, exaggeration, temperature, cfgw, min_p, top_p, repetition_penalty, max_new_tokens, use_long_text, chunk_method, max_chunk_size, optimize_memory):\n", "    try:\n", "        # The model is already loaded in the notebook's global state\n", "        audio_prompt_path = audio_prompt if audio_prompt else None\n", "        \n", "        # Estimate memory usage for long texts\n", "        if len(text) > 500:\n", "            memory_info = model.estimate_memory_usage(text, int(max_new_tokens))\n", "            print(f\"Memory estimate: {memory_info['total_estimated_mb']:.0f}MB, recommended chunk size: {memory_info['recommended_chunk_size']}\")\n", "        \n", "        # Choose generation method based on text length and user preference\n", "        if use_long_text or len(text) > max_chunk_size:\n", "            # Use long text generation with chunking\n", "            wav = model.generate_long_text(\n", "                text=text,\n", "                chunk_method=chunk_method,\n", "                max_chunk_size=int(max_chunk_size),\n", "                audio_prompt_path=audio_prompt_path,\n", "                exaggeration=exaggeration,\n", "                temperature=temperature,\n", "                cfg_weight=cfgw,\n", "                min_p=min_p,\n", "                top_p=top_p,\n", "                repetition_penalty=repetition_penalty,\n", "                max_new_tokens=int(max_new_tokens),\n", "                optimize_memory_between_chunks=optimize_memory,\n", "            )\n", "        else:\n", "            # Use standard generation for shorter texts\n", "            # Check if the generate method supports max_new_tokens parameter\n", "            generate_params = inspect.signature(model.generate).parameters\n", "            \n", "            if 'max_new_tokens' in generate_params:\n", "                wav = model.generate(\n", "                    text,\n", "                    audio_prompt_path=audio_prompt_path,\n", "                    exaggeration=exaggeration,\n", "                    temperature=temperature,\n", "                    cfg_weight=cfgw,\n", "                    min_p=min_p,\n", "                    top_p=top_p,\n", "                    repetition_penalty=repetition_penalty,\n", "                    max_new_tokens=int(max_new_tokens),\n", "                )\n", "            else:\n", "                # Fallback for older version without max_new_tokens\n", "                wav = model.generate(\n", "                    text,\n", "                    audio_prompt_path=audio_prompt_path,\n", "                    exaggeration=exaggeration,\n", "                    temperature=temperature,\n", "                    cfg_weight=cfgw,\n", "                    min_p=min_p,\n", "                    top_p=top_p,\n", "                    repetition_penalty=repetition_penalty,\n", "                )\n", "        \n", "        return (model.sr, wav.squeeze(0).numpy()), None\n", "    except Exception as e:\n", "        return None, str(e)\n", "\n", "with gr.Blocks() as demo:\n", "    with gr.Row():\n", "        with gr.Column():\n", "            text = gr.Textbox(\n", "                value=\"Now let's make my mum's favourite. So three mars bars into the pan. Then we add the tuna and just stir for a bit, just let the chocolate and fish infuse. A sprinkle of olive oil and some tomato ketchup. Now smell that. Oh boy this is going to be incredible.\",\n", "                label=\"Text to synthesize\",\n", "                max_lines=5\n", "            )\n", "            ref_wav = gr.Audio(sources=[\"upload\", \"microphone\"], type=\"filepath\", label=\"Reference Audio File\", value=None)\n", "            exaggeration = gr.Slider(0.25, 2, step=.05, label=\"Exaggeration (Neutral = 0.5, extreme values can be unstable)\", value=.5)\n", "            cfg_weight = gr.Slider(0.0, 1, step=.05, label=\"CFG/Pace\", value=0.5)\n", "\n", "            with gr.Accordion(\"Long Text Options\", open=False):\n", "                use_long_text = gr.Checkbox(label=\"Force Long Text Mode (for ultra-long texts)\", value=False)\n", "                chunk_method = gr.Dropdown([\"sentences\", \"clauses\", \"character\"], label=\"Chunking Method\", value=\"sentences\")\n", "                max_chunk_size = gr.Slider(50, 1000, step=50, label=\"Max Chunk Size (characters)\", value=200)\n", "                optimize_memory = gr.Checkbox(label=\"Optimize Memory Between Chunks\", value=True)\n", "\n", "            with gr.Accordion(\"More options\", open=False):\n", "                temp = gr.Slider(0.05, 5, step=.05, label=\"temperature\", value=.8)\n", "                min_p = gr.Slider(0.00, 1.00, step=0.01, label=\"min_p || Newer Sampler. Recommend 0.02 > 0.1. Handles Higher Temperatures better. 0.00 Disables\", value=0.05)\n", "                top_p = gr.Slider(0.00, 1.00, step=0.01, label=\"top_p || Original Sampler. 1.0 Disables(recommended). Original 0.8\", value=1.00)\n", "                repetition_penalty = gr.Slider(1.00, 2.00, step=0.1, label=\"repetition_penalty\", value=1.2)\n", "                max_new_tokens = gr.Slider(100, 2000, step=50, label=\"Max New Tokens\", value=1000)\n", "\n", "            run_btn = gr.Button(\"Generate\", variant=\"primary\")\n", "\n", "        with gr.Column():\n", "            audio_output = gr.Audio(label=\"Output Audio\")\n", "            error_output = gr.Textbox(label=\"Error\", visible=False)\n", "\n", "    def show_error(error_message):\n", "        if error_message:\n", "            return gr.update(visible=True, value=error_message)\n", "        return gr.update(visible=False)\n", "\n", "    run_btn.click(\n", "        fn=generate_with_gradio,\n", "        inputs=[\n", "            text,\n", "            ref_wav,\n", "            exaggeration,\n", "            temp,\n", "            cfg_weight,\n", "            min_p,\n", "            top_p,\n", "            repetition_penalty,\n", "            max_new_tokens,\n", "            use_long_text,\n", "            chunk_method,\n", "            max_chunk_size,\n", "            optimize_memory,\n", "        ],\n", "        outputs=[audio_output, error_output],\n", "    ).then(show_error, inputs=error_output, outputs=error_output)\n", "\n", "demo.launch(share=True)"]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}