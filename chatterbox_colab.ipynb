{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["# Chatterbox TTS - Colab Notebook\n",
    "This notebook demonstrates how to use Chatterbox, a state-of-the-art open-source TTS model. Follow the steps below to get started."]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 1. Install Dependencies\n",
    "First, let's install the required packages."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["!pip install chatterbox-tts gradio"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 2. Import Libraries and Setup"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["import torchaudio as ta\n",
    "import torch\n",
    "from chatterbox.tts import ChatterboxTTS\n",
    "\n",
    "# Automatically detect the best available device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 3. Load the Model\n",
    "Now we'll load the pre-trained Chatterbox TTS model."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["model = ChatterboxTTS.from_pretrained(device=device)"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 4. Generate Speech from Text\n",
    "Let's generate some speech using the default voice."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["text = \"Hello! This is a test of the Chatterbox TTS system. It can generate natural-sounding speech from text.\"\n",
    "wav = model.generate(text)\n",
    "\n",
    "# Save the generated audio\n",
    "output_path = \"test_output.wav\"\n",
    "ta.save(output_path, wav, model.sr)\n",
    "\n",
    "# Display audio player in the notebook\n",
    "from IPython.display import Audio\n",
    "Audio(output_path)"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 5. Voice Cloning (Optional)\n",
    "You can also clone a voice by providing an audio prompt. Upload your audio file and specify its path below."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Upload your audio file and set its path here\n",
    "AUDIO_PROMPT_PATH = \"path_to_your_audio.wav\"  # Replace with your audio file path\n",
    "\n",
    "# Generate speech with the voice from the audio prompt\n",
    "text = \"This is the same text but spoken in a different voice.\"\n",
    "wav = model.generate(text, audio_prompt_path=AUDIO_PROMPT_PATH)\n",
    "\n",
    "# Save and play the generated audio\n",
    "output_path_cloned = \"test_output_cloned.wav\"\n",
    "ta.save(output_path_cloned, wav, model.sr)\n",
    "Audio(output_path_cloned)"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 6. Long Text Generation Example"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["# Example of generating audio from long text with memory optimization\n", "long_text = \"\"\"\n", "This is a demonstration of the new long text generation capabilities. \n", "The system can now handle ultra-long texts by breaking them into smaller chunks. \n", "Each chunk is processed separately to avoid memory issues. \n", "The chunks are then combined to create seamless audio output. \n", "This approach allows for generating hours of audio content without running out of memory. \n", "The chunking can be done by sentences, clauses, or character count. \n", "Sentence-based chunking preserves natural speech patterns for the best quality. \n", "Memory optimization between chunks ensures efficient resource usage. \n", "You can also estimate memory usage before generation to plan accordingly.\n", "\"\"\"\n", "\n", "# Estimate memory usage\n", "memory_info = model.estimate_memory_usage(long_text)\n", "print(f\"Text length: {memory_info['text_length']} characters\")\n", "print(f\"Estimated memory usage: {memory_info['total_estimated_mb']:.0f}MB\")\n", "print(f\"Recommended chunk size: {memory_info['recommended_chunk_size']} characters\")\n", "\n", "# Generate audio using long text method\n", "wav_long = model.generate_long_text(\n", "    text=long_text,\n", "    chunk_method=\"sentences\",\n", "    max_chunk_size=200,\n", "    optimize_memory_between_chunks=True\n", ")\n", "\n", "# Save and play the generated audio\n", "output_path_long = \"test_output_long.wav\"\n", "ta.save(output_path_long, wav_long, model.sr)\n", "Audio(output_path_long)"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 7. Gradio UI with Live Server"]"}]}}}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["import gradio as gr\n", "import inspect\n", "\n", "def generate_with_gradio(text, audio_prompt, exaggeration, temperature, cfgw, min_p, top_p, repetition_penalty, max_new_tokens, use_long_text, chunk_method, max_chunk_size, optimize_memory):\n", "    try:\n", "        # The model is already loaded in the notebook's global state\n", "        audio_prompt_path = audio_prompt if audio_prompt else None\n", "        \n", "        # Estimate memory usage for long texts\n", "        if len(text) > 500:\n", "            memory_info = model.estimate_memory_usage(text, int(max_new_tokens))\n", "            print(f\"Memory estimate: {memory_info['total_estimated_mb']:.0f}MB, recommended chunk size: {memory_info['recommended_chunk_size']}\")\n", "        \n", "        # Choose generation method based on text length and user preference\n", "        if use_long_text or len(text) > max_chunk_size:\n", "            # Use long text generation with chunking\n", "            wav = model.generate_long_text(\n", "                text=text,\n", "                chunk_method=chunk_method,\n", "                max_chunk_size=int(max_chunk_size),\n", "                audio_prompt_path=audio_prompt_path,\n", "                exaggeration=exaggeration,\n", "                temperature=temperature,\n", "                cfg_weight=cfgw,\n", "                min_p=min_p,\n", "                top_p=top_p,\n", "                repetition_penalty=repetition_penalty,\n", "                max_new_tokens=int(max_new_tokens),\n", "                optimize_memory_between_chunks=optimize_memory,\n", "            )\n", "        else:\n", "            # Use standard generation for shorter texts\n", "            # Check if the generate method supports max_new_tokens parameter\n", "            generate_params = inspect.signature(model.generate).parameters\n", "            \n", "            if 'max_new_tokens' in generate_params:\n", "                wav = model.generate(\n", "                    text,\n", "                    audio_prompt_path=audio_prompt_path,\n", "                    exaggeration=exaggeration,\n", "                    temperature=temperature,\n", "                    cfg_weight=cfgw,\n", "                    min_p=min_p,\n", "                    top_p=top_p,\n", "                    repetition_penalty=repetition_penalty,\n", "                    max_new_tokens=int(max_new_tokens),\n", "                )\n", "            else:\n", "                # Fallback for older version without max_new_tokens\n", "                wav = model.generate(\n", "                    text,\n", "                    audio_prompt_path=audio_prompt_path,\n", "                    exaggeration=exaggeration,\n", "                    temperature=temperature,\n", "                    cfg_weight=cfgw,\n", "                    min_p=min_p,\n", "                    top_p=top_p,\n", "                    repetition_penalty=repetition_penalty,\n", "                )\n", "        \n", "        return (model.sr, wav.squeeze(0).numpy()), None\n", "    except Exception as e:\n", "        return None, str(e)\n", "\n", "with gr.Blocks() as demo:\n", "    with gr.Row():\n", "        with gr.Column():\n", "            text = gr.Textbox(\n", "                value=\"Now let's make my mum's favourite. So three mars bars into the pan. Then we add the tuna and just stir for a bit, just let the chocolate and fish infuse. A sprinkle of olive oil and some tomato ketchup. Now smell that. Oh boy this is going to be incredible.\",\n", "                label=\"Text to synthesize\",\n", "                max_lines=5\n", "            )\n", "            ref_wav = gr.Audio(sources=[\"upload\", \"microphone\"], type=\"filepath\", label=\"Reference Audio File\", value=None)\n", "            exaggeration = gr.Slider(0.25, 2, step=.05, label=\"Exaggeration (Neutral = 0.5, extreme values can be unstable)\", value=.5)\n", "            cfg_weight = gr.Slider(0.0, 1, step=.05, label=\"CFG/Pace\", value=0.5)\n", "\n", "            with gr.Accordion(\"Long Text Options\", open=False):\n", "                use_long_text = gr.Checkbox(label=\"Force Long Text Mode (for ultra-long texts)\", value=False)\n", "                chunk_method = gr.Dropdown([\"sentences\", \"clauses\", \"character\"], label=\"Chunking Method\", value=\"sentences\")\n", "                max_chunk_size = gr.Slider(50, 1000, step=50, label=\"Max Chunk Size (characters)\", value=200)\n", "                optimize_memory = gr.Checkbox(label=\"Optimize Memory Between Chunks\", value=True)\n", "\n", "            with gr.Accordion(\"More options\", open=False):\n", "                temp = gr.Slider(0.05, 5, step=.05, label=\"temperature\", value=.8)\n", "                min_p = gr.Slider(0.00, 1.00, step=0.01, label=\"min_p || Newer Sampler. Recommend 0.02 > 0.1. Handles Higher Temperatures better. 0.00 Disables\", value=0.05)\n", "                top_p = gr.Slider(0.00, 1.00, step=0.01, label=\"top_p || Original Sampler. 1.0 Disables(recommended). Original 0.8\", value=1.00)\n", "                repetition_penalty = gr.Slider(1.00, 2.00, step=0.1, label=\"repetition_penalty\", value=1.2)\n", "                max_new_tokens = gr.Slider(100, 2000, step=50, label=\"Max New Tokens\", value=1000)\n", "\n", "            run_btn = gr.Button(\"Generate\", variant=\"primary\")\n", "\n", "        with gr.Column():\n", "            audio_output = gr.Audio(label=\"Output Audio\")\n", "            error_output = gr.Textbox(label=\"Error\", visible=False)\n", "\n", "    def show_error(error_message):\n", "        if error_message:\n", "            return gr.update(visible=True, value=error_message)\n", "        return gr.update(visible=False)\n", "\n", "    run_btn.click(\n", "        fn=generate_with_gradio,\n", "        inputs=[\n", "            text,\n", "            ref_wav,\n", "            exaggeration,\n", "            temp,\n", "            cfg_weight,\n", "            min_p,\n", "            top_p,\n", "            repetition_penalty,\n", "            max_new_tokens,\n", "            use_long_text,\n", "            chunk_method,\n", "            max_chunk_size,\n", "            optimize_memory,\n", "        ],\n", "        outputs=[audio_output, error_output],\n", "    ).then(show_error, inputs=error_output, outputs=error_output)\n", "\n", "demo.launch(share=True)"]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}